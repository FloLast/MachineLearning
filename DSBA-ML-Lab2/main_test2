import numpy as np
import matplotlib.pyplot as plt
import scipy.optimize as op

data = loadtxt('data1.txt', delimiter=',')
 
X = data[:, 0:2]
y = data[:, 2]

def sigmoid(z):
    return 1/(1+np.exp(-z))

def computeCost(theta, X, y): 
    # Computes the cost using theta as the parameter 
    # for logistic regression. 
    
    m = X.shape[0] # number of training examples
    
    J = 0
    
    # ====================== YOUR CODE HERE ======================
    # Instructions: Calculate the error J of the decision boundary
    #               that is described by theta (see the assignment 
    #				for more details).
    
    
    J=-sum(y*np.log(sigmoid(np.dot(theta,X.T)).T)+(1-y)*np.log(1-sigmoid(np.dot(theta,X.T)).T))/m

    
    
    
    # =============================================================
    
    return J

def computeGrad(theta, X, y):
    # Computes the gradient of the cost with respect to
    # the parameters.
    
    m = X.shape[0] # number of training examples
    
    grad = np.zeros(np.size(theta)) # initialize gradient
    
    # ====================== YOUR CODE HERE ======================
    # Instructions: Compute the gradient of cost for each theta,
    # as described in the assignment.
    
    grad=np.dot(sigmoid(np.dot(theta,X.T)).T-y,X)/m
    
    
    
    
    
    
    
    # =============================================================

    return grad


def predict(theta, X):
# Predict whether the label is 0 or 1 using learned logistic 
# regression parameters theta. The threshold is set at 0.5

    m = X.shape[0] # number of training examples

    c = np.zeros(m) # predicted classes of training examples

    p = np.zeros(m) # logistic regression outputs of training examples


    # ====================== YOUR CODE HERE ======================
    # Instructions: Predict the label of each instance of the
    #training set.
    
    p=sigmoid(dot(X,theta))
    for i in range(len(c)):
        if p[i]>0.5:
            c[i]=1
        else:
            c[i]=0
    
    
    
    
    
    
    # =============================================================
    
    return c

#Add intercept term to X
X_new = ones((X.shape[0], 3))
X_new[:, 1:3] = X
X = X_new

# Initialize fitting parameters
initial_theta = zeros((3,1))

# Run minimize() to obtain the optimal theta
Result = op.minimize(fun = computeCost, x0 = initial_theta, args = (X, y), method = 'TNC',jac = computeGrad);
theta = Result.x;

# Plot the decision boundary
plot_x = array([min(X[:, 1]) - 2, max(X[:, 2]) + 2])
plot_y = (- 1.0 / theta[2]) * (theta[1] * plot_x + theta[0])
plt.plot(plot_x, plot_y)
plt.scatter(X[pos, 1], X[pos, 2], marker='o', c='b')
plt.scatter(X[neg, 1], X[neg, 2], marker='x', c='r')
plt.xlabel('Exam 1 score')
plt.ylabel('Exam 2 score')
plt.legend(['Decision Boundary', 'Admitted', 'Not Admitted'])
plt.show()

# Compute accuracy on the training set
p = predict(array(theta), X)
counter = 0
for i in range(y.size):
    if p[i] == y[i]:
        counter += 1
print 'Train Accuracy: %f' % (counter / float(y.size) * 100.0)